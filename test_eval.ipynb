{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_file = \"evaluation_results/gpt4o-judge_result_20250220_q_QnA_car_sample_14_test.xlsx\"\n",
    "final_df = pd.read_excel(result_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 평가 결과 요약 ===\n",
      "\n",
      "메소드별 평가 결과:\n",
      "                                              method  correctness_score  \\\n",
      "0         hybrid_api+bm25_HyDE_full_page_no_reranker             0.3571   \n",
      "1         hybrid_sts+bm25_HyDE_full_page_no_reranker             0.5000   \n",
      "2           hybrid_sts+bm25_HyDE_full_page_rerank_20             0.5714   \n",
      "3         hybrid_sts+bm25_None_full_page_no_reranker             0.5000   \n",
      "4           hybrid_sts+bm25_None_full_page_rerank_20             0.4286   \n",
      "5  hybrid_sts+bm25_QueryRewrite_full_page_no_rera...             0.6429   \n",
      "6   hybrid_sts+bm25_QueryRewrite_full_page_rerank_20             0.5000   \n",
      "\n",
      "   relevance_score  grounded_score  retrieval_relevance_score  precision  \\\n",
      "0           0.6429          0.4286                     0.9286     0.1477   \n",
      "1           0.5714          0.4286                     0.7143     0.2050   \n",
      "2           0.7143          0.4286                     0.7857     0.0942   \n",
      "3           0.6429          0.5000                     0.7857     0.1015   \n",
      "4           0.5714          0.5000                     0.7857     0.1011   \n",
      "5           0.7143          0.6429                     0.7857     0.1510   \n",
      "6           0.5714          0.5000                     0.7857     0.0900   \n",
      "\n",
      "   recall      f1  rougeL    bleu  fail_rate  \n",
      "0  0.3156  0.1827  0.2848  0.2209     0.2143  \n",
      "1  0.3858  0.2401  0.3326  0.2713     0.3571  \n",
      "2  0.2797  0.1244  0.2336  0.1913     0.2143  \n",
      "3  0.2823  0.1374  0.2513  0.2081     0.2857  \n",
      "4  0.2625  0.1343  0.2395  0.2081     0.3571  \n",
      "5  0.4468  0.2081  0.2895  0.2320     0.1429  \n",
      "6  0.3389  0.1270  0.2382  0.1881     0.4286  \n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== 평가 결과 요약 ===\")\n",
    "\n",
    "# 평가할 메트릭 정의\n",
    "metrics = {\n",
    "    'correctness_score': lambda x: (x == True).mean(),\n",
    "    'relevance_score': lambda x: (x == True).mean(),\n",
    "    'grounded_score': lambda x: (x == True).mean(),\n",
    "    'retrieval_relevance_score': lambda x: (x == True).mean(),\n",
    "    'precision': 'mean',\n",
    "    'recall': 'mean',\n",
    "    'f1': 'mean',\n",
    "    'rougeL': 'mean',\n",
    "    'bleu': 'mean',\n",
    "    'fail_rate': 'mean'\n",
    "}\n",
    "\n",
    "# groupby를 사용하여 한 번에 계산\n",
    "result_df = final_df.groupby('method').agg(metrics).reset_index()\n",
    "\n",
    "# 결과 저장\n",
    "result_df.to_excel(\"output.xlsx\", index=False)\n",
    "\n",
    "# 결과 출력 (선택사항)\n",
    "print(\"\\n메소드별 평가 결과:\")\n",
    "print(result_df.round(4))  # 소수점 4자리까지 표시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
