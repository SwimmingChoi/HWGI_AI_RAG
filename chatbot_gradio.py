# [2íŒ€] 2íŒ€ ì½”ë“œ - Gradio ì±—ë´‡ ì½”ë“œ

from modules.qa_chain import QAChain
from modules.config_loader import load_config
from modules.sparse_retrieval import BM25Retriever
from langchain.retrievers import EnsembleRetriever
from modules.dense_retrieval import CustomEmbeddings
from modules.document_processor import DocumentProcessor
from utils.logger import setup_logger
from modules.faq import FAQ

import os
import torch
import argparse
import gradio as gr
import logging
import json

# [2íŒ€] Gradio ì±—ë´‡ ì‹¤í–‰ (ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ë‹µë³€ì„ ì œê³µí•©ë‹ˆë‹¤. ë¡œì§ì€ rag_main.pyì™€ ë™ì¼í•©ë‹ˆë‹¤.)
def main():
    
    parser = argparse.ArgumentParser(description='Enter user name for the config script')
    parser.add_argument('--config', type=str, required=False, default='config')
    args = parser.parse_args()

    current_dir = os.path.dirname(os.path.abspath(__file__))
    config = load_config(os.path.join(current_dir, "config", f"{args.config}.yaml"))
    logger = setup_logger()
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

    print("general insurace uploading....")


    # [2íŒ€] STEP 1: ì…ë ¥ document ì²˜ë¦¬
    doc_processor = DocumentProcessor()
    docs, metadata = doc_processor.load_and_process_documents(
        file_path=config['dataset']['document'],
        mode=config['dataset']['document_type'],
        do_chunking=config['preprocessing']['chunking'],
        chunk_size=config['preprocessing']['chunk_size'],
        chunk_overlap=config['preprocessing']['chunk_overlap']
    )

    # [2íŒ€] STEP 2: FAQ ë°ì´í„°ì…‹ (jsoníŒŒì¼) ë‚´ì—ì„œ ì§ˆë¬¸ë“¤ë§Œ ì¶”ì¶œí•˜ì—¬ question_dictionary ë¦¬ìŠ¤íŠ¸ì— ì €ì¥
    # â†’ ì´í›„ ì‚¬ìš©ìì˜ ì§ˆë¬¸ê³¼ FAQ ë°ì´í„°ì…‹ ë‚´ ì§ˆë¬¸ì„ ë¹„êµí•˜ê¸° ìœ„í•´ ì‚¬ìš©
    faq_data_path=os.path.join(current_dir, "documents", config['dataset']['faq'])
    with open(faq_data_path, 'r') as f:
        faq_data = json.load(f)
    question_dictionary=[]
    for i in range(len(faq_data)):
        question_dictionary.append(faq_data[i]['original_question'])
        
        
    # [2íŒ€] STEP 3: Hybrid Retriever ì„¤ì •
    retrieversp = BM25Retriever(docs) 
    faiss_index_path = config['dataset']['document'].replace(
        '.json',
        f"_{config['dense_model']['model_type']}_" +
        ('chunk.bin' if config['preprocessing']['chunking'] else 'full.bin')
    )
    embedding_service = CustomEmbeddings(
        model_type=config['dense_model']['model_type'],
        model_path=config['dense_model']['model_cache'],
        device=device,
        embedding_config=config,
        faiss_index_path=faiss_index_path,
    )
    retrieverde = embedding_service.create_or_load_vectorstore(docs)
    retrieverde = retrieverde.as_retriever(search_kwargs={
        "k": config['retrieval']['top_k'],
        "search_type": config['retrieval']['search_type']
    })
    retriever_hybrid = EnsembleRetriever(
        retrievers=[retrieversp, retrieverde],
        weights=[0.4, 0.6])
    print("retrieval type saved")


    # [2íŒ€] STEP 4: QA chain ì„¤ì •
    qa_chain = QAChain(
        openai_config=config['openai'],
        retriever= retriever_hybrid,
        search_type=config['retrieval']['type'], 
        query_translation_type=config['query_translation']['type'],
        top_k= config['retrieval']['top_k'],
        use_reranker=config['retrieval']['reranker'],
        reranker_model=config['retrieval']['reranker_model'],
        reranker_top_k=config['retrieval']['reranker_top_k'],
        num = config['multi_step_iteration']['num']
    )
    print("qa-chain saved")
    
    
     # [2íŒ€] STEP 5: ê³ ê°ìƒë‹´ì„¼í„° ì—°ë½ì²˜ ì €ì¥
     # â†’'ëª¨ë¥´ê² ìŠµë‹ˆë‹¤' ë‹µë³€ ìƒì„± ê²½ìš°ë¥¼ ëŒ€ë¹„í•˜ì—¬ ì¤€ë¹„í•˜ì˜€ìŒ
    hanwha=f'''
í•œí™”ì†í•´ë³´í—˜ ê³ ê°ìƒë‹´ì„¼í„°(1566-8000)ë¡œ ë¬¸ì˜í•´ì£¼ì„¸ìš”.
<u>í•œí™”ì†í•´ë³´í—˜ ê³ ê°ìƒë‹´ì„¼í„° ë°”ë¡œê°€ê¸°</u>
'''

    def chat(message, history):
        logger.info(f"ì‚¬ìš©ìê°€ ì§ˆë¬¸ì„ ì…ë ¥í–ˆìŠµë‹ˆë‹¤.")
        """
        [2íŒ€] ì‚¬ìš©ì ì§ˆë¬¸ì„ ì²˜ë¦¬í•˜ê³  ì±—ë´‡ì˜ ë‹µë³€ì„ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜
        
        Parameters:
            message (str): ì‚¬ìš©ì ì§ˆë¬¸
            history (list): ì´ì „ ëŒ€í™” ê¸°ë¡
            
        Returns:
            list: ì—…ë°ì´íŠ¸ëœ ëŒ€í™” ê¸°ë¡ (ì‚¬ìš©ì ì§ˆë¬¸ê³¼ ì±—ë´‡ ë‹µë³€ í¬í•¨)
        """
        
        # [2íŒ€] STEP 6: FAQ ë°ì´í„°ì…‹ ì§ˆë¬¸ë“¤ ì¤‘ ì‚¬ìš©ì ì…ë ¥ ì§ˆë¬¸ê³¼ ì˜ë¯¸ì ìœ¼ë¡œ ë™ì¼í•œ ì§ˆë¬¸ì´ ìˆëŠ”ì§€ í™•ì¸
        infaq=FAQ(api_key=config['openai']['api_key'])
        answer_faq = infaq.run_faq(question=message, question_dictionary=question_dictionary, faq_data=faq_data)
        # [2íŒ€] STEP 7: FAQ ë°ì´í„°ì…‹ ë‚´ ì‚¬ìš©ìì˜ ì§ˆë¬¸ê³¼ ì˜ë¯¸ì ìœ¼ë¡œ ë™ì¼í•œ ì§ˆë¬¸ì´ ìˆëŠ” ê²½ìš° ë‹µë³€ ìƒì„±
        if answer_faq!=False:
            logger.info(f"FAQ ë°ì´í„°ì…‹ì„ í™œìš©í•˜ì—¬ ë‹µë³€ ìƒì„±ì„ ì™„ë£Œí–ˆìŠµë‹ˆë‹¤.")
            return history + [[message, answer_faq]]
        
        # [2íŒ€] STEP 8: FAQ ë°ì´í„°ì…‹ ë‚´ ì‚¬ìš©ìì˜ ì§ˆë¬¸ê³¼ ì˜ë¯¸ì ìœ¼ë¡œ ë™ì¼í•œ ì§ˆë¬¸ì´ ì—†ëŠ” ê²½ìš° RAG ì‹¤í–‰
        else:
            total_result = qa_chain.multi_step_qa(message, reset_memory=True)
            answer_result=total_result['answer']
            logger.info(f"RAG ì‹¤í–‰ í›„, ë‹µë³€ ìƒì„±ì„ ì™„ë£Œí–ˆìŠµë‹ˆë‹¤.")        
        # [2íŒ€] STEP9: ë‹µë³€ ìƒì„±
        # â†’'ëª¨ë¥´ê² ìŠµë‹ˆë‹¤'ê°€ ë‹µë³€ ë‚´ì— ì¡´ì¬í•˜ëŠ” ê²½ìš°, ê³ ê°ìƒë‹´ì„¼í„° ì—°ë½ì²˜ë„ ì œê³µ
        # â†’'ëª¨ë¥´ê² ìŠµë‹ˆë‹¤'ê°€ ë‹µë³€ ë‚´ì— ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ê²½ìš°, ìƒì„±ëœ ë‹µë³€ë§Œ ì œê³µ
        # â†’ ì§ˆë¬¸ê³¼ ë‹µë³€ì„ historyë¡œ ì €ì¥
            if 'ëª¨ë¥´ê² ìŠµë‹ˆë‹¤' in answer_result:
                return history + [[message, answer_result+hanwha]]
            else:
                return history + [[message, answer_result]]
    
    # [2íŒ€] Gradio ì¸í„°í˜ì´ìŠ¤
    with gr.Blocks(css="""
.message {
  font-size: 1.2em !important;
}

.message-content {
  font-size: 1.2em !important;
}

.message-wrap {
  max-width: 100% !important;
  padding: 0 !important;
  margin: 0 !important;
}

/* ì‚¬ìš©ìì™€ ë´‡ ë©”ì‹œì§€ ê³µí†µ ìŠ¤íƒ€ì¼ */
.message.user, 
.message.bot {
  padding: 0.5rem !important;
  margin: 0 !important;
}

/* ì‚¬ìš©ì ë©”ì‹œì§€ ì •ë ¬ */
.chat .user-message {
  justify-content: flex-end !important;
  padding: 0 !important;
  margin: 0 !important;
}

.chat .user-message .message-wrap {
  margin-left: auto !important;
}

/* ë©”ì‹œì§€ ë ˆì´ë¸” ê³µí†µ ìŠ¤íƒ€ì¼ */
.message.bot::before,
.message.user::before {
  display: block;
  margin-bottom: 0.7rem;
  font-size: 0.6em;
  color: #666;
}

/* ë´‡ ë©”ì‹œì§€ ë ˆì´ë¸” */
.message.bot::before {
  content: "ğŸ¤–í•œí™”ì†í•´ë³´í—˜ ì±—ë´‡";
}

/* ì‚¬ìš©ì ë©”ì‹œì§€ ë ˆì´ë¸” */
.message.user::before {
  content: "ğŸ¤·ê³ ê°ë‹˜";
  text-align: right;
}
    """) as demo:
        gr.Markdown("<div align='center'>\n\n# ğŸš— í•œí™” ìë™ì°¨ ë³´í—˜\n\n</div>")
        gr.Markdown("<div align='center'>\n\n## ğŸ¤–ì•ˆë…•í•˜ì„¸ìš”. í•œí™” ìë™ì°¨ ë³´í—˜ ì±—ë´‡ì…ë‹ˆë‹¤. ì˜¤ëŠ˜ë„ ì¹œì ˆí•˜ê²Œ ë‹µë³€ë“œë¦¬ê² ìŠµë‹ˆë‹¤!\n\n</div>")
        
        chatbot = gr.Chatbot(height=400)
        msg = gr.Textbox(label="ê¶ê¸ˆí•˜ì‹  ì ì´ ìˆë‹¤ë©´ ì–¸ì œë“  í¸í•˜ê²Œ ì—¬ì­¤ë³´ì„¸ìš”.", placeholder="ì—¬ê¸°ì— ë©”ì‹œì§€ë¥¼ ì…ë ¥í•˜ì„¸ìš”...")
        clear = gr.Button("ëŒ€í™”ë‚´ìš© ì „ì²´ ì‚­ì œ")
        
        msg.submit(chat, [msg, chatbot], [chatbot]).then(
            lambda: "", None, [msg]
        )
        
        clear.click(lambda: None, None, chatbot, queue=False)
    
    demo.launch()

if __name__ == "__main__":
    main()
