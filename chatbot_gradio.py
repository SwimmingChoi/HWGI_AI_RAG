from modules.qa_chain import QAChain
from modules.config_loader import load_config
from modules.sparse_retrieval import BM25Retriever
from langchain.retrievers import EnsembleRetriever
from modules.dense_retrieval import CustomEmbeddings
from modules.document_processor import DocumentProcessor

import os
import torch
import argparse
import gradio as gr


def main():
    
    parser = argparse.ArgumentParser(description='Enter user name for the config script')
    parser.add_argument('--config', type=str, required=False, default='config')
    args = parser.parse_args()

    current_dir = os.path.dirname(os.path.abspath(__file__))
    config = load_config(os.path.join(current_dir, "config", f"{args.config}.yaml"))
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

    print("general insurace uploading....")


    # ë¬¸ì„œ ì²˜ë¦¬
    doc_processor = DocumentProcessor()
    docs, metadata = doc_processor.load_and_process_documents(
        file_path=config['dataset']['document'],
        mode=config['dataset']['document_type'],
        do_chunking=config['preprocessing']['chunking'],
        chunk_size=config['preprocessing']['chunk_size'],
        chunk_overlap=config['preprocessing']['chunk_overlap']
    )


    # hybrid retriever
    retrieversp = BM25Retriever(docs) #tokenizer=config['retrieval']['tokenizer']
    faiss_index_path = config['dataset']['document'].replace(
        '.json',
        f"_{config['dense_model']['model_type']}_" +
        ('chunk.bin' if config['preprocessing']['chunking'] else 'full.bin')
    )
    embedding_service = CustomEmbeddings(
        model_type=config['dense_model']['model_type'],
        model_path=config['dense_model']['model_cache'],
        device=device,
        embedding_config=config,
        faiss_index_path=faiss_index_path,
    )
    retrieverde = embedding_service.create_or_load_vectorstore(docs)
    retrieverde = retrieverde.as_retriever(search_kwargs={
        "k": config['retrieval']['top_k'],
        "search_type": config['retrieval']['search_type']
    })
    retriever_hybrid = EnsembleRetriever(
        retrievers=[retrieversp, retrieverde],
        weights=[0.4, 0.6])
    print("retrieval type saved")


    # qa-chain
    qa_chain = QAChain(
        openai_config=config['openai'],
        retriever= retriever_hybrid,
        search_type=config['retrieval']['type'], 
        query_translation_type=config['query_translation']['type'],
        top_k= config['retrieval']['top_k'],
        use_reranker=config['retrieval']['reranker'],
        reranker_model=config['retrieval']['reranker_model'],
        reranker_top_k=config['retrieval']['reranker_top_k']
    )
    print("qa-chain saved")
    
    hanwha=f'''
í•œí™”ì†í•´ë³´í—˜ ê³ ê°ìƒë‹´ì„¼í„°(1566-8000)ë¡œ ë¬¸ì˜í•´ì£¼ì„¸ìš”.
<u>í•œí™”ì†í•´ë³´í—˜ ê³ ê°ìƒë‹´ì„¼í„° ë°”ë¡œê°€ê¸°</u>
'''

    def chat(message, history):
        """
        ì‚¬ìš©ì ì§ˆë¬¸ì„ ì²˜ë¦¬í•˜ê³  ì±—ë´‡ì˜ ë‹µë³€ì„ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜
        
        Parameters:
            message (str): ì‚¬ìš©ì ì§ˆë¬¸
            history (list): ì´ì „ ëŒ€í™” ê¸°ë¡
            
        Returns:
            list: ì—…ë°ì´íŠ¸ëœ ëŒ€í™” ê¸°ë¡ (ì‚¬ìš©ì ì§ˆë¬¸ê³¼ ì±—ë´‡ ë‹µë³€ í¬í•¨)
        """
        total_result = qa_chain.process_question(message, reset_memory=True)
        answer_result=total_result['answer']
        if 'ëª¨ë¥´ê² ìŠµë‹ˆë‹¤' in answer_result:
            return history + [[message, answer_result+hanwha]]
        else:
            return history + [[message, answer_result]]
    
    # ëŒ€í™” ì¸í„°í˜ì´ìŠ¤
    with gr.Blocks(css="""
        .message { font-size: 1.2em !important; }
        .message-wrap { 
            max-width: 100% !important;
            padding: 0 !important;
            margin: 0 !important;
        }
        .message.user, .message.bot { 
            padding: 0.5rem !important;
            margin: 0 !important;
        }
        .message-content { font-size: 1.2em !important; }
        .chat .user-message { 
            justify-content: flex-end !important;
            padding: 0 !important;
            margin: 0 !important;
        }
        .chat .user-message .message-wrap { margin-left: auto !important; }
        
        .message.bot::before {
            content: "ğŸ¤–í•œí™”ì†í•´ë³´í—˜ ì±—ë´‡";
            display: block;
            margin-bottom: 0.7rem;
            font-size: 0.6em;
            color: #666;
        }
        
        .message.user::before {
            content: "ğŸ¤·ê³ ê°ë‹˜";
            display: block;
            margin-bottom: 0.7rem;
            font-size: 0.6em;
            color: #666;
            text-align: right;
        }
    """) as demo:
        gr.Markdown("<div align='center'>\n\n# ğŸš— í•œí™” ìë™ì°¨ ë³´í—˜\n\n</div>")
        gr.Markdown("<div align='center'>\n\n## ğŸ¤–ì•ˆë…•í•˜ì„¸ìš”. í•œí™” ìë™ì°¨ ë³´í—˜ ì±—ë´‡ì…ë‹ˆë‹¤. ì˜¤ëŠ˜ë„ ì¹œì ˆí•˜ê²Œ ë‹µë³€ë“œë¦¬ê² ìŠµë‹ˆë‹¤!\n\n</div>")
        
        chatbot = gr.Chatbot(height=400)
        msg = gr.Textbox(label="ê¶ê¸ˆí•˜ì‹  ì ì´ ìˆë‹¤ë©´ ì–¸ì œë“  í¸í•˜ê²Œ ì—¬ì­¤ë³´ì„¸ìš”.", placeholder="ì—¬ê¸°ì— ë©”ì‹œì§€ë¥¼ ì…ë ¥í•˜ì„¸ìš”...")
        clear = gr.Button("ëŒ€í™”ë‚´ìš© ì „ì²´ ì‚­ì œ")
        
        msg.submit(chat, [msg, chatbot], [chatbot]).then(
            lambda: "", None, [msg]
        )
        
        clear.click(lambda: None, None, chatbot, queue=False)
    
    demo.launch()

if __name__ == "__main__":
    main()
